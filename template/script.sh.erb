#!/usr/bin/env bash

set -euo pipefail

# Allow callers to override base directories via environment variables.
HOST_TEXT_LAB_DIR="${TEXT_LAB_DIR:-/storage/research/dsl_shared/solutions/ondemand/text_lab}"

CONT_TEXT_LAB_DIR="/text_lab"

#point to the apptainer containers on research storage
TL_CONTAINER="$HOST_TEXT_LAB_DIR/container/textlab_MB.sif"

#point to where the ollama models are stored on research storage
HOST_OLLAMA_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/ollama"
CONT_OLLAMA_DIR="/opt/ollama"

#point to where the whisper models are stored on research storage
HOST_WHISPER_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/whisper"
CONT_WHISPER_DIR="/opt/whisper"

# point to where whisperx cache is stored on research storage
HOST_WHISPERX_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/whisperx"
CONT_WHISPERX_DIR="/opt/whisperx"

#point to where torch models are stored on research storage
HOST_TORCH_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/torch"
CONT_TORCH_DIR="/opt/torch"

# point to where huggingface models are stored on research storage
HOST_HF_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/huggingface"
CONT_HF_DIR="/opt/huggingface"

# point to where OCR caches are stored on research storage
HOST_EASYOCR_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/easyocr"
CONT_EASYOCR_DIR="/opt/easyocr"

# PaddleX/PaddleOCR ignore env vars in some versions and default to ~/.paddlex/.paddleocr
HOST_PADDLEOCR_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/paddleocr"
HOST_PADDLEX_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/paddlex"
CONT_PADDLEOCR_DIR="$HOME/.paddleocr"
CONT_PADDLEX_DIR="$HOME/.paddlex"

export OLLAMA_MODELS="$CONT_OLLAMA_DIR/models"

# ---------------------------- Runtime parameters ------------------------------
# Which ports should each service listen on?
TEXT_LAB_PORT="${TEXT_LAB_PORT:-${SERVER_PORT:-8502}}"   # streamlit defaults to 8502 if not set
OLM_OCR_PORT="${OLMOCR_PORT:-8503}"

# Base URL path for reverseâ€‘proxy setups (JupyterHub, etc.). Leave empty for /
SERVER_BASEURLPATH="${SERVER_BASEURLPATH:-}"  # may already be exported by the hub

# --- Clean the Python environment ---
unset PYTHONPATH PYTHONHOME
export PYTHONNOUSERSITE=1

# Explicitly set HOME and USER for environment isolation
export HOME="$HOME"
export USER="$USER"

pushd "${PWD}"


# If CUDA_VISIBLE_DEVICES is empty, unmask GPU 0 
if [ -z "${CUDA_VISIBLE_DEVICES:-}" ]; then
  export CUDA_VISIBLE_DEVICES=0
fi

apptainer exec \
  --nv \
  --bind "$HOST_TEXT_LAB_DIR:$CONT_TEXT_LAB_DIR" \
  --bind "$HOST_OLLAMA_DIR:$CONT_OLLAMA_DIR:rw" \
  --bind /storage:/storage \
  --bind "$HOST_WHISPER_DIR:$CONT_WHISPER_DIR:rw" \
  --bind "$HOST_WHISPERX_DIR:$CONT_WHISPERX_DIR:rw" \
  --bind "$HOST_TORCH_DIR:$CONT_TORCH_DIR:rw" \
  --bind "$HOST_HF_DIR:$CONT_HF_DIR:rw" \
  --bind "$HOST_EASYOCR_DIR:$CONT_EASYOCR_DIR:rw" \
  --bind "$HOST_PADDLEOCR_DIR:$CONT_PADDLEOCR_DIR:rw" \
  --bind "$HOST_PADDLEX_DIR:$CONT_PADDLEX_DIR:rw" \
  --env HF_HOME=/opt/huggingface \
  --env PADDLEX_HOME="$CONT_PADDLEX_DIR" \
  --env PADDLEOCR_CACHE="$CONT_PADDLEOCR_DIR" \
  --env VLLM_GPU_MEMORY_UTILIZATION=0.6 \
  --env OLLAMA_MODELS="$CONT_OLLAMA_DIR/models" \
  --env OLLAMA_LLM_LIBRARY="cuda" \
  --env OLLAMA_FLASH_ATTENTION="true" \
  --env VLLM_GPU_MEMORY_UTILIZATION=0.6 \
  "$TL_CONTAINER" \
  /usr/bin/python3 -m streamlit run "/storage/research/dsl_shared/solutions/ondemand/text_lab/src_main/src/Home.py" \
    --server.address 0.0.0.0 \
    --server.port "$TEXT_LAB_PORT" \
    --server.baseUrlPath "$SERVER_BASEURLPATH" \
    --server.headless true \
    --server.maxUploadSize 10000 \
    --browser.serverAddress ondemand.hpc.unibe.ch \
    --browser.gatherUsageStats false \
    --logger.level error \
    --client.showErrorDetails false 

popd
