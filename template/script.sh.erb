#!/usr/bin/env bash

set -euo pipefail

# Allow callers to override base directories via environment variables.
HOST_TEXT_LAB_DIR="${TEXT_LAB_DIR:-/storage/research/dsl_shared/solutions/ondemand/text_lab}"

echo "HOST_TEXT_LAB_DIR: $HOST_TEXT_LAB_DIR"

CONT_TEXT_LAB_DIR="/text_lab"
CONT_OLMOCR_DIR="/olmocr"

TL_CONTAINER="$HOST_TEXT_LAB_DIR/container/text_lab.sif"
OCR_CONTAINER="$HOST_TEXT_LAB_DIR/container/olmocr.sif"

export OCR_CONTAINER

# ---------------------------- Runtime parameters ------------------------------
# Which ports should each service listen on?
TEXT_LAB_PORT="${TEXT_LAB_PORT:-${SERVER_PORT:-8502}}"   # streamlit defaults to 8502 if not set
OLM_OCR_PORT="${OLMOCR_PORT:-8503}"

# Base URL path for reverse‑proxy setups (JupyterHub, etc.). Leave empty for /
SERVER_BASEURLPATH="${SERVER_BASEURLPATH:-}"  # may already be exported by the hub

# --- Clean the Python environment ---
unset PYTHONPATH PYTHONHOME
export PYTHONNOUSERSITE=1

# Explicitly set HOME and USER for environment isolation
export HOME="$HOME"
export USER="$USER"

#set the variable for whisper to use models cached from inside the container
export WHISPER_CACHE="/opt/whisper"

pushd "${PWD}"

# choose one – example with shared scratch
MODEL_ROOT="/storage/homefs/$USER/ollama_models"
# MODEL_ROOT="$SLURM_TMPDIR/ollama_models"
mkdir -p "$MODEL_ROOT"

apptainer exec \
  --bind "$HOST_TEXT_LAB_DIR:$CONT_TEXT_LAB_DIR" \
  --bind "$MODEL_ROOT:/ollama_models" \
  --env OLLAMA_MODELS="/ollama_models" \
  --nv \
  "$TL_CONTAINER" \
  /usr/bin/python3 -m streamlit run "$CONT_TEXT_LAB_DIR/src/Home.py" \
    --server.port "$TEXT_LAB_PORT" \
    --server.baseUrlPath "$SERVER_BASEURLPATH" \
    --server.enableCORS false \
    --server.headless true \
    --server.maxUploadSize 500 \
    --client.showErrorDetails false

popd
