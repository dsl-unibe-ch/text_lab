#!/usr/bin/env bash

set -euo pipefail

# Allow callers to override base directories via environment variables.
HOST_TEXT_LAB_DIR="${TEXT_LAB_DIR:-/storage/research/dsl_shared/solutions/ondemand/text_lab}"

CONT_TEXT_LAB_DIR="/text_lab"

#point to the apptainer containers on research storage
TL_CONTAINER="$HOST_TEXT_LAB_DIR/container/text_lab_170226.sif"

#point to where the ollama models are stored on research storage
HOST_OLLAMA_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/ollama"
CONT_OLLAMA_DIR="/opt/ollama"

#point to where the whisper models are stored on research storage
HOST_WHISPER_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/whisper"
CONT_WHISPER_DIR="/opt/whisper"

# point to where whisperx cache is stored on research storage
HOST_WHISPERX_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/whisperx"
CONT_WHISPERX_DIR="/opt/whisperx"

#point to where torch models are stored on research storage
HOST_TORCH_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/torch"
CONT_TORCH_DIR="/opt/torch"

# point to where huggingface models are stored on research storage
HOST_HF_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/huggingface"
CONT_HF_DIR="/opt/huggingface"

# point to where OCR caches are stored on research storage
HOST_EASYOCR_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/easyocr"
CONT_EASYOCR_DIR="/opt/easyocr"

# PaddleX/PaddleOCR ignore env vars in some versions and default to ~/.paddlex/.paddleocr
HOST_PADDLEOCR_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/paddleocr"
HOST_PADDLEX_DIR="/storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/paddlex"
CONT_PADDLEOCR_DIR="$HOME/.paddleocr"
CONT_PADDLEX_DIR="$HOME/.paddlex"
PADDLE_PDX_CACHE_HOME="${PADDLE_PDX_CACHE_HOME:-$HOST_PADDLEX_DIR}"

export OLLAMA_MODELS="$CONT_OLLAMA_DIR/models"

# ---------------------------- Runtime parameters ------------------------------
# Which ports should each service listen on?
TEXT_LAB_PORT="${TEXT_LAB_PORT:-${SERVER_PORT:-8502}}"   # streamlit defaults to 8502 if not set
OLM_OCR_PORT="${OLMOCR_PORT:-8503}"
OLMOCR_GPU_MEMORY_UTILIZATION="${OLMOCR_GPU_MEMORY_UTILIZATION:-0.6}"

# Base URL path for reverseâ€‘proxy setups (JupyterHub, etc.). Leave empty for /
SERVER_BASEURLPATH="${SERVER_BASEURLPATH:-}"  # may already be exported by the hub

# --- Clean the Python environment ---
unset PYTHONPATH PYTHONHOME
export PYTHONNOUSERSITE=1

# Explicitly set HOME and USER for environment isolation
export HOME="$HOME"
export USER="$USER"

pushd "${PWD}"


# If CUDA_VISIBLE_DEVICES is empty, unmask GPU 0 
if [ -z "${CUDA_VISIBLE_DEVICES:-}" ]; then
  export CUDA_VISIBLE_DEVICES=0
fi

# ... previous exports and logic ...

# Use bash -c to run multiple commands: 
# 1. Start ollama serve in the background (&)
# 2. Wait 5-10 seconds for it to initialize
# 3. Run streamlit
# ... (Keep all your previous variable definitions up to the pushd/popd) ...

mkdir -p "$HOST_TEXT_LAB_DIR/ollama_tmp"

apptainer exec \
  --nv \
  --bind "$HOST_TEXT_LAB_DIR:$CONT_TEXT_LAB_DIR" \
  --bind "$HOST_OLLAMA_DIR:$CONT_OLLAMA_DIR:rw" \
  --bind /storage:/storage \
  --bind "$HOST_WHISPER_DIR:$CONT_WHISPER_DIR:rw" \
  --bind "$HOST_WHISPERX_DIR:$CONT_WHISPERX_DIR:rw" \
  --bind "$HOST_TORCH_DIR:$CONT_TORCH_DIR:rw" \
  --bind "$HOST_HF_DIR:$CONT_HF_DIR:rw" \
  --bind "$HOST_EASYOCR_DIR:$CONT_EASYOCR_DIR:rw" \
  --bind "$HOST_PADDLEOCR_DIR:$CONT_PADDLEOCR_DIR:rw" \
  --bind "$HOST_PADDLEX_DIR:$CONT_PADDLEX_DIR:rw" \
  --env HF_HOME=/opt/huggingface \
  --env PADDLEX_HOME="$CONT_PADDLEX_DIR" \
  --env PADDLEOCR_CACHE="$CONT_PADDLEOCR_DIR" \
  --env PADDLE_PDX_CACHE_HOME="$PADDLE_PDX_CACHE_HOME" \
  --env VLLM_GPU_MEMORY_UTILIZATION="$OLMOCR_GPU_MEMORY_UTILIZATION" \
  --env OLMOCR_GPU_MEMORY_UTILIZATION="$OLMOCR_GPU_MEMORY_UTILIZATION" \
  --env OLLAMA_MODELS="$CONT_OLLAMA_DIR/models" \
  --env OLLAMA_FLASH_ATTENTION="true" \
  "$TL_CONTAINER" \
  bash -c "export OLLAMA_TMPDIR=$CONT_TEXT_LAB_DIR/ollama_tmp && \
           export TMPDIR=$CONT_TEXT_LAB_DIR/ollama_tmp && \
           ollama serve > $CONT_TEXT_LAB_DIR/ollama_server.log 2>&1 & \
           sleep 5 && \
           /usr/bin/python3 -m streamlit run '/storage/research/dsl_shared/solutions/ondemand/text_lab/src_main/src/Home.py' \
             --server.address 0.0.0.0 \
             --server.port '$TEXT_LAB_PORT' \
             --server.baseUrlPath '$SERVER_BASEURLPATH' \
             --server.headless true \
             --server.maxUploadSize 10000 \
             --browser.serverAddress ondemand.hpc.unibe.ch \
             --browser.gatherUsageStats false \
             --logger.level error \
             --client.showErrorDetails false"

popd
