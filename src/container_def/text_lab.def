Bootstrap: docker
From: nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

###############################################################################
# 0.  Runtime variables (visible at RUN-time, not just build-time)
###############################################################################
%environment
    export WHISPER_CACHE=/opt/whisper
    export OLLAMA_MODELS=/opt/ollama/models
    export TRANSFORMERS_CACHE=/opt/huggingface

###############################################################################
# 1.  Build steps
###############################################################################
%post
    set -e

    ###########################################################################
    # 1-A  Define/cache dirs for the *build* stage
    ###########################################################################
    WHISPER_CACHE=/opt/whisper
    OLLAMA_MODELS=/opt/ollama/models
    TRANSFORMERS_CACHE=/opt/huggingface
    export WHISPER_CACHE OLLAMA_MODELS TRANSFORMERS_CACHE

    mkdir -p "${WHISPER_CACHE}" "${OLLAMA_MODELS}" "${TRANSFORMERS_CACHE}"
    chmod -R a+rX "${WHISPER_CACHE}" "${OLLAMA_MODELS}" "${TRANSFORMERS_CACHE}"

    ###########################################################################
    # 1-B  OS packages + Python stack
    ###########################################################################
    apt update && \
    apt upgrade -y && \
    apt install -y --no-install-recommends \
        build-essential curl ffmpeg git \
        libsndfile1-dev \
        libc6 libc-bin pciutils lshw libpq-dev \
        software-properties-common python3.10 python3-pip

    python3 -m pip install --no-cache-dir --upgrade pip setuptools
    apt remove -y python3-blinker || true
    pip3 uninstall -y blinker || true

    python3 -m pip install --no-cache-dir \
        sentence-transformers \
        elevenlabs \
        streamlit \
        streamlit-cookies-manager \
        openai-whisper \
        scikit-learn \
        umap-learn \
        matplotlib \
        pandas \
        seaborn \
        mlflow \
        datasets \
        requests \
        soundfile \
        opencv-python-headless \
        scikit-image \
        ollama

    # Torch (CUDA 12.1)
    pip3 uninstall -y torch torchvision torchaudio || true
    pip3 install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 \
        --index-url https://download.pytorch.org/whl/cu121

    ###########################################################################
    # 2.  Pre-download Whisper checkpoints
    ###########################################################################
    python3 - << 'PY'
import os, whisper
cache_dir = os.environ["WHISPER_CACHE"]
for name in ["tiny", "small", "medium", "large-v2", "large-v3", "turbo"]:
    print(f"---> downloading Whisper model: {name}")
    whisper.load_model(name, download_root=cache_dir)
print("All Whisper checkpoints saved to", cache_dir)
PY

    ###########################################################################
    # 3.  Install Ollama & pull the selected LLM weights
    ###########################################################################
    curl -fsSL https://ollama.com/install.sh | sh

    echo ">>> starting Ollama server in the background"
    ollama serve > /tmp/ollama-build.log 2>&1 &
    OLLAMA_PID=$!



    sleep 2

    for m in \
        gemma3:27b \
        deepseek-r1:8b \
        deepseek-r1:14b \
        deepseek-r1:70b \
        llama3.2:latest \
        llama3.1:latest \
        qwen2.5:32b \
        qwen3:32b ; do
        echo "---> pulling Ollama model: ${m}"
        ollama pull "${m}" || echo "WARNING: pull failed for ${m}"
    done


    # --- shut the server down ----------------------------------------------
    kill "$OLLAMA_PID" 2>/dev/null || true
    wait "$OLLAMA_PID" 2>/dev/null || true
    echo ">>> Ollama server stopped"

    ###########################################################################
    # 4.  Quick sanity snapshots (first 10 lines to keep log short)
    ###########################################################################
    echo "=== Whisper cache snapshot ==="
    ls -lh "${WHISPER_CACHE}" | head
    echo "=== Ollama models snapshot ==="
    ls -lh "${OLLAMA_MODELS}" | head

    ###########################################################################
    # 5.  Clean-up
    ###########################################################################
    apt clean && rm -rf /var/lib/apt/lists/*
    echo "Build finished – all models cached under /opt."

###############################################################################
# 2.  Default runscript – just exec whatever command is passed
###############################################################################
%runscript
    exec "$@"