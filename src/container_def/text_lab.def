Bootstrap: docker
From: nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

%environment
    # External model locations (bind-mount at runtime)
    export WHISPER_CACHE=/opt/whisper            # bind to shared whisper weights
    export OLLAMA_MODELS=/opt/ollama/models      # bind to shared ollama models

    # Optional HF cache if needed later
    export TRANSFORMERS_CACHE=/opt/huggingface

    # Ollama runtime libs path (for the CLI binary we install below)
    export LD_LIBRARY_PATH=/usr/local/lib/ollama:$LD_LIBRARY_PATH

%post
    set -e
    export DEBIAN_FRONTEND=noninteractive

    # Create/cache dirs expected at runtime (will be empty in the image)
    mkdir -p /opt/whisper /opt/ollama/models /opt/huggingface
    chmod -R a+rX /opt/whisper /opt/ollama /opt/huggingface

    # Accept MS fonts EULA
    echo "ttf-mscorefonts-installer msttcorefonts/accepted-mscorefonts-eula select true" | debconf-set-selections

    # OS packages + Python
    apt update && \
    apt upgrade -y && \
    apt install -y --no-install-recommends \
        build-essential curl ffmpeg git libsndfile1-dev \
        pciutils lshw libpq-dev \
        python3.10 python3-pip \
        poppler-utils ttf-mscorefonts-installer msttcorefonts \
        fonts-crosextra-caladea fonts-crosextra-carlito gsfonts \
        lcdf-typetools libgl1 ca-certificates software-properties-common

    # --- INSTALL APPTAINER to run other containers ---
    add-apt-repository -y ppa:apptainer/ppa
    apt update
    apt install -y apptainer
    

    # Python bootstrap
    python3 -m pip install --no-cache-dir --upgrade pip setuptools
    # Some stacks ship blinker that can clash with streamlit on some images
    apt remove -y python3-blinker || true
    pip3 install --upgrade pip
    pip3 uninstall -y blinker || true

    # Core app + DS stack
    python3 -m pip install --no-cache-dir \
        streamlit \
        streamlit-cookies-manager \
        streamlit-autorefresh \
        requests \
        openai-whisper \
        sentence-transformers \
        scikit-learn \
        umap-learn \
        matplotlib \
        pandas \
        seaborn \
        plotly \
        polars \
        pyarrow \
        openpyxl \
        mlflow \
        datasets \
        soundfile \
        opencv-python-headless \
        scikit-image \
        ollama \
        mcp[cli] \
        openai \
        tabulate

    # Torch (CUDA 12.1)
    pip3 uninstall -y torch torchvision torchaudio || true
    pip3 install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 \
        --index-url https://download.pytorch.org/whl/cu121

    # Ollama CLI binary (+ its CUDA libs) so we can run `ollama serve`
    OLM_VER="0.13.0"
    TMP=/tmp/ollama_pkgs && mkdir -p "$TMP" /usr/local/lib/ollama
    cd "$TMP"
    curl -fL "https://github.com/ollama/ollama/releases/download/v${OLM_VER}/ollama-linux-amd64.tgz" -o ollama.tgz
    tar -xzf ollama.tgz
    install -m 0755 bin/ollama /usr/local/bin/ollama
    cp -r lib/ollama/* /usr/local/lib/ollama/ || true
    echo "/usr/local/lib/ollama" > /etc/ld.so.conf.d/ollama.conf
    ldconfig
    rm -rf "$TMP"

    # Clean-up
    apt clean && rm -rf /var/lib/apt/lists/*
    echo "Build finished â€“ Text Lab runtime ready (models are external)."

%runscript
    exec "$@"

%help
External model binds required at runtime:
  /storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/whisper  -> /opt/whisper:ro
  /storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/ollama   -> /opt/ollama:ro
(Optional)
  /storage/research/dsl_shared/solutions/ondemand/text_lab/container/models/hf_cache -> /opt/huggingface:ro