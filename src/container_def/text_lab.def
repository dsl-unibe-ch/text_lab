Bootstrap: docker
From: nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

###############################################################################
# 0.  Runtime variables (visible at RUN-time, not just build-time)
###############################################################################
%environment
    export WHISPER_CACHE=/opt/whisper
    export OLLAMA_MODELS=/opt/ollama/models
    export TRANSFORMERS_CACHE=/opt/huggingface

###############################################################################
# 1.  Build steps
###############################################################################
%post
    set -e

    ###########################################################################
    # 1.1  Define/cache dirs for the *build* stage
    ###########################################################################
    WHISPER_CACHE=/opt/whisper
    OLLAMA_MODELS=/opt/ollama/models
    TRANSFORMERS_CACHE=/opt/huggingface
    export WHISPER_CACHE OLLAMA_MODELS TRANSFORMERS_CACHE

    mkdir -p "${WHISPER_CACHE}" "${OLLAMA_MODELS}" "${TRANSFORMERS_CACHE}"
    chmod -R a+rX "${WHISPER_CACHE}" "${OLLAMA_MODELS}" "${TRANSFORMERS_CACHE}"

    ###########################################################################
    # 1.2 Inner Apptainer setup
    ###########################################################################
    # Set non-interactive frontend for Debconf
    export DEBIAN_FRONTEND=noninteractive

    # Accept the EULA for installing ttf-mscorefonts-installer
    echo "ttf-mscorefonts-installer msttcorefonts/accepted-mscorefonts-eula select true" | debconf-set-selections

    # Installing software-properties-common to be able to run add-apt-repository
    apt update && \
    apt upgrade -y && \
    apt install -y --no-install-recommends software-properties-common

    # Add Apptainer repository to be able to install Apptainer
    add-apt-repository -y ppa:apptainer/ppa

    ###########################################################################
    # 1.3  OS packages + Python stack
    ###########################################################################
    apt update && \
    apt upgrade -y && \
    apt install -y --no-install-recommends \
        build-essential curl ffmpeg git libsndfile1-dev libc6 libc-bin pciutils lshw libpq-dev python3.10 python3-pip \
        poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts \
        lcdf-typetools apptainer

    python3 -m pip install --no-cache-dir --upgrade pip setuptools
    apt remove -y python3-blinker || true
    pip3 install --upgrade pip
    pip3 uninstall -y blinker || true

    python3 -m pip install --no-cache-dir \
        sentence-transformers \
        elevenlabs \
        streamlit \
        streamlit-cookies-manager \
        openai-whisper \
        scikit-learn \
        umap-learn \
        matplotlib \
        pandas \
        seaborn \
        mlflow \
        datasets \
        requests \
        soundfile \
        opencv-python-headless \
        scikit-image \
        ollama

    # Torch (CUDA 12.1)
    pip3 uninstall -y torch torchvision torchaudio || true
    pip3 install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 \
        --index-url https://download.pytorch.org/whl/cu121

    ###########################################################################
    # 1.4  Pre-download Whisper checkpoints
    ###########################################################################
    python3 - << 'PY'
import os, whisper
cache_dir = os.environ["WHISPER_CACHE"]
for name in ["tiny", "small", "medium", "large-v2", "large-v3", "turbo"]:
    print(f"---> downloading Whisper model: {name}")
    whisper.load_model(name, download_root=cache_dir)
print("All Whisper checkpoints saved to", cache_dir)
PY

    ###########################################################################
    # 1.5  Install Ollama & pull the selected LLM weights
    ###########################################################################
    curl -fsSL https://ollama.com/install.sh | sh

    echo ">>> starting Ollama server in the background"
    ollama serve > /tmp/ollama-build.log 2>&1 &
    OLLAMA_PID=$!

    sleep 2

    for m in \
        gemma3:12b \
        gemma3:27b \
        deepseek-r1:8b \
        deepseek-r1:14b \
        deepseek-r1:70b \
        llama3.2:latest \
        llama3.1:latest \
        qwen2.5:32b; do
        echo "---> pulling Ollama model: ${m}"
        ollama pull "${m}" || echo "WARNING: pull failed for ${m}"
    done


    # --- shut the server down ----------------------------------------------
    kill "$OLLAMA_PID" 2>/dev/null || true
    wait "$OLLAMA_PID" 2>/dev/null || true
    echo ">>> Ollama server stopped"

    ###########################################################################
    # 1.6  Quick sanity snapshots (first 10 lines to keep log short)
    ###########################################################################
    echo "=== Whisper cache snapshot ==="
    ls -lh "${WHISPER_CACHE}" | head
    echo "=== Ollama models snapshot ==="
    ls -lh "${OLLAMA_MODELS}" | head

    ###########################################################################
    # 1.7  Clean-up
    ###########################################################################
    apt clean && rm -rf /var/lib/apt/lists/*
    echo "Build finished – all models cached under /opt."

###############################################################################
# 2.  Default runscript – just exec whatever command is passed
###############################################################################
%runscript
    exec "$@"