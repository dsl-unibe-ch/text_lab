# pages/Visualize_Data.py
import os
import sys
import io
import json
import zipfile
import tempfile
import traceback
import asyncio
from pathlib import Path

import streamlit as st

# --- Streamlit page config ---
st.set_page_config(page_title="Visualize Data", layout="wide")

# Make project root importable (so we can import auth.check_token)
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from auth import check_token  # noqa: E402

check_token()

# --- MCP / stdio plumbing ---
from mcp import ClientSession, StdioServerParameters, types  # noqa: E402
from mcp.client.stdio import stdio_client  # noqa: E402

# Paths
PROJECT_ROOT = Path(__file__).resolve().parent.parent
MCP_SERVER_PATH = str(PROJECT_ROOT / "mcp_server.py")
MCP_ARTIFACTS_DIR = os.environ.get("MCP_ARTIFACTS_DIR", str(PROJECT_ROOT / "mcp_artifacts"))


# ------------------------- Async helpers & error handling -------------------------
def run_async(coro):
    """Run an async coroutine safely under Streamlit."""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    if loop.is_running():
        # Rare under Streamlit, but guard anyway
        return asyncio.run_coroutine_threadsafe(coro, loop).result()
    return loop.run_until_complete(coro)


def _first_exception_msg(exc: BaseException) -> str:
    """Extract a readable message from ExceptionGroup/TaskGroup errors."""
    if hasattr(exc, "exceptions"):  # py>=3.11 ExceptionGroup or anyio groups
        try:
            inner = exc.exceptions[0]
            return _first_exception_msg(inner)
        except Exception:
            pass
    return f"{exc.__class__.__name__}: {exc}"


def _read_json_content(tool_result) -> dict | list:
    """
    Read MCP tool result that may be:
      - TextContent (containing JSON text), or
      - Json-like content on newer MCPs (has .type in {'json','application/json'} and .data)
    Falls back to parsing .text as JSON. Raises if unsupported.
    """
    if not tool_result or not getattr(tool_result, "content", None):
        raise RuntimeError("Empty tool result content")

    item = tool_result.content[0]

    # --- Newer MCPs: JsonContent or json-like payloads (duck-typed) ---
    # Handle both the real class and "json-ish" objects without importing JsonContent.
    item_type = getattr(item, "type", None)  # e.g., 'json', 'application/json', None
    if item_type in {"json", "application/json"} and hasattr(item, "data"):
        return item.data  # already a Python object

    # --- Older MCPs: TextContent with JSON string ---
    # Note: TextContent exists in all versions we've seen.
    from mcp import types as _types
    if isinstance(item, _types.TextContent):
        txt = item.text or ""
        import json
        try:
            return json.loads(txt)
        except Exception as e:
            raise RuntimeError(f"Failed to parse JSON from TextContent: {e}\nText was: {txt[:200]}")

    # --- Last resort: try to parse any '.text' field as JSON string ---
    txt = getattr(item, "text", None)
    if isinstance(txt, str):
        import json
        try:
            return json.loads(txt)
        except Exception:
            pass

    raise RuntimeError(f"Unsupported MCP content type: {type(item).__name__} (no JSON data or text)")


# --------------------------- ZIP bundling utilities ---------------------------
async def _build_zip(dataset_id: str, summary_payload: dict) -> bytes:
    """
    Zips all files under mcp_artifacts/<dataset_id>, plus a summary.json and README.txt.
    Returns bytes.
    """
    base_dir = Path(MCP_ARTIFACTS_DIR) / dataset_id
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        # Include all generated artifacts (plots, etc.)
        if base_dir.exists():
            for p in base_dir.rglob("*"):
                if p.is_file():
                    arcname = str(Path(dataset_id) / p.relative_to(base_dir))
                    zf.write(p, arcname=arcname)

        # Add summary.json
        zf.writestr(f"{dataset_id}/summary.json", json.dumps(summary_payload, ensure_ascii=False, indent=2))

        # Add a small README
        readme = (
            "# Data Visualization Bundle\n\n"
            "- `summary.json`: schema, suggestions, and generated plots metadata\n"
            "- `plots/`: PNG charts generated by auto_explore\n"
            f"- Source dataset id: `{dataset_id}`\n"
        )
        zf.writestr(f"{dataset_id}/README.txt", readme)

    buf.seek(0)
    return buf.read()


# ------------------------------ Core workflow ------------------------------
async def _register_and_explore(file_path: str, max_plots: int = 6) -> dict:
    """
    Launch MCP server (stdio), register dataset, fetch head/schema/suggestions,
    run auto_explore, collect plot paths, and return data for UI.
    """
    server_params = StdioServerParameters(
        command=sys.executable,
        args=[MCP_SERVER_PATH],
        env={"MCP_ARTIFACTS_DIR": MCP_ARTIFACTS_DIR, **os.environ},
    )

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()

            # 1) Register dataset
            reg = await session.call_tool("register_dataset_from_file", arguments={"file_path": file_path})
            ds = _read_json_content(reg)
            dataset_id = ds.get("dataset_id")
            if not dataset_id:
                raise RuntimeError(f"register_dataset_from_file returned no dataset_id. Payload: {ds}")

            # 2) Head & schema
            head_res = await session.call_tool("head", arguments={"dataset_id": dataset_id, "n": 10})
            schema_res = await session.call_tool("schema", arguments={"dataset_id": dataset_id})
            head_json = _read_json_content(head_res)
            schema_json = _read_json_content(schema_res)

            # 3) Suggestions
            sugg_res = await session.call_tool("suggest_charts", arguments={"dataset_id": dataset_id, "max_suggestions": 12})
            suggestions = _read_json_content(sugg_res)

            # 4) Auto-explore
            auto_res = await session.call_tool("auto_explore", arguments={"dataset_id": dataset_id, "max_plots": max_plots})
            auto_json = _read_json_content(auto_res)

            image_paths = [
                item.get("path")
                for item in auto_json.get("generated", [])
                if isinstance(item, dict) and item.get("path")
            ]

            summary_payload = {
                "dataset_id": dataset_id,
                "shape": ds.get("shape"),
                "columns": ds.get("columns"),
                "head": head_json,
                "schema": schema_json,
                "suggestions": suggestions,
                "generated": auto_json.get("generated", []),
                "skipped": auto_json.get("skipped", []),
            }

            zip_bytes = await _build_zip(dataset_id, summary_payload)

            return {
                "dataset_id": dataset_id,
                "preview_markdown": ds.get("preview_markdown", ""),
                "head": head_json,
                "schema": schema_json,
                "suggestions": suggestions,
                "images": image_paths,
                "zip_bytes": zip_bytes,
                "artifacts_dir": str(Path(MCP_ARTIFACTS_DIR) / dataset_id),
            }


# --------------------------------- UI ---------------------------------
st.title("Visualize Data")

st.markdown(
    "Upload a tabular file (CSV / TSV / Parquet / JSON). "
    "We’ll analyze its schema and automatically create a small set of useful plots."
)

left, right = st.columns([2, 1])

with left:
    uploaded = st.file_uploader(
        "Upload file",
        type=["csv", "tsv", "txt", "parquet", "json", "jsonl", "data"],
        help="We try to auto-detect delimiter for CSV/TSV/TXT."
    )

with right:
    max_plots = st.number_input("Max plots", min_value=1, max_value=20, value=6, step=1)

# Persist last result in session
if "viz_result" not in st.session_state:
    st.session_state["viz_result"] = None

if st.button("Process"):
    if not uploaded:
        st.warning("Please upload a file first.")
    else:
        # Save upload to a temp path (keep extension for delimiter sniffing)
        suffix = f".{uploaded.name.split('.')[-1]}" if "." in uploaded.name else ".csv"
        with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:
            tmp.write(uploaded.read())
            tmp.flush()
            tmp_path = tmp.name

        with st.spinner("Processing your data with the MCP server (analyzing & plotting)…"):
            try:
                result = run_async(_register_and_explore(tmp_path, max_plots=max_plots))
                st.session_state["viz_result"] = result
                st.success("Done! Scroll down to see the preview, schema, and plots. You can also download the ZIP.")
            except Exception as e:
                st.error(_first_exception_msg(e))
                with st.expander("Full traceback"):
                    st.code("".join(traceback.format_exception(e)))

# Render results if present
res = st.session_state.get("viz_result")
if res:
    st.subheader("Preview (first rows)")
    # Prefer head() preview if present, else the initial preview_markdown from register
    head_md = (res.get("head") or {}).get("markdown")
    preview_md = head_md or res.get("preview_markdown") or "_(no preview)_"
    st.markdown(preview_md)

    with st.expander("Schema details"):
        st.json(res.get("schema", {}))

    with st.expander("Chart suggestions"):
        st.json(res.get("suggestions", []))

    images = res.get("images", [])
    if images:
        st.subheader("Generated plots")
        cols = st.columns(2)
        for i, p in enumerate(images):
            try:
                cols[i % 2].image(p, caption=os.path.basename(p), use_container_width=True)
            except Exception:
                pass
    else:
        st.info("No plots generated.")

    st.subheader("Download all results")
    st.download_button(
        label="Download ZIP",
        data=res["zip_bytes"],
        file_name="visualization_bundle.zip",
        mime="application/zip",
    )

    st.caption(f"Artifacts folder: `{res['artifacts_dir']}`")
